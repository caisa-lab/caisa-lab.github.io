# 2025-2026 Perspective: From Making AI Work to Making It Matter

The turn of this year feels different—because a longer arc has become clear. The past five to seven years of AI research were largely driven by one question: how to make large language models work at all. How to scale them, stabilize them, and push their capabilities far enough to matter.

In many respects, that phase has been successful. LLMs now do work—in research, in industry, and increasingly in everyday settings.

What changes now is the question. The next phase is no longer about whether these systems work, but about what we do with them once they do. How do we integrate them into human workflows responsibly? How do we make them robust, interpretable, and trustworthy under real-world uncertainty? And how do we embed them into science and public institutions in ways that scale—and last?

This shift—from making AI work to deciding how it should work for people and science—is what connects our recent results and what will guide our work over the next 5–7 years.

Almost everything my group has been building converges on two tightly connected research lines:

**Socially Aligned Artificial Intelligence**, and

**AI for Accelerating Scientific Discovery**, especially in physics.

What binds them is a shared methodological core: robustness, efficiency, and interpretability of foundation models. These properties are not optional. They are what make AI systems trustworthy in social settings and usable as scientific instruments. Methods, people, and questions move back and forth between the two lines. Crucially, this work is embedded in shared, centralized infrastructures for open AI science at HPC scale, rather than isolated projects.

### **Socially Aligned Artificial Intelligence** 

Our Social AI work asks how AI systems model people, social interaction, and society—and how these capabilities can be measured, interpreted, and governed responsibly. A major milestone this year was the ERC Starting Grant “LLMpathy”, funded by the European Research Council.

What excites me most is not the label, but what it enables: social intelligence becomes a measurable research object. With LLMpathy, we can simulate, stress-test, and systematically analyze social reasoning in large language models, rather than relying on anecdotal behavior or narrow benchmarks. This builds directly on our completed junior research group Dynamically Social Discourse Analysis and recent ACL and EMNLP publications.

Equally important is the community forming around this work. The approval of a Dagstuhl Seminar on “Social Artificial Intelligence” for 2026 and the upcoming ACM CHI workshop “Redefining Empathy” signal that the field is ready for deeper, more reflective conversations. For me, socially aligned AI is inseparable from AI safety and long-term resilience: the question is not if AI will shape human systems, but how we design that integration to remain human-centered and sustainable.

### **AI for Scientific Discovery** 

Our work on AI for Scientific Discovery connects directly to alignment and safety. This is not an analogy: both depend on transparent, uncertainty-aware, and interpretable methods. Physics, in particular, is unforgiving. Models must operate under uncertainty, cope with distribution shifts, and integrate into workflows where assumptions are constantly challenged. In that sense, AI for physics is not downstream of alignment—it is one of its most demanding testbeds.

A concrete example was the ECML PKDD Challenge “Colliding with Adversaries”, which created a shared evaluation space for machine learning and physics researchers and made robustness issues—such as correlation attacks—visible in a way that mattered to both communities.

This line is anchored in the Dynaverse Excellence Cluster, where I serve as PI for AI for Astrophysics, and reinforced by BMBF ErUM-Data collaborations—AISafety, AALearning, and Physics-LLM—linking us with partners at Bonn, RWTH Aachen, TU Dortmund, DESY, Forschungszentrum Jülich, and the Leibniz Institute for Astrophysics Potsdam. Complementary DFG-funded work on hyperbolic representations and our embedding in the ELLIS network add methodological depth and international structure.

Looking ahead, this line will increasingly focus on physics foundation models—self-supervised, physics-aware models that generalize across detectors, experiments, and regimes—and on LLM-based scientific agents that assist with literature-driven hypothesis generation, equation extraction, tool-augmented reasoning with simulators, and provenance-aware workflows.

### **Open-Source Foundation Models** 

As chair of [**Lamarr NLP**](https://lamarr-institute.org/research/natural-language-processing/), I'm happy about a third strand that matured significantly - our work on open-source foundation models. For me, this is about making values like transparency, reproducibility, and controllability operational. I’ve been particularly happy about how tightly research, collaboration, and infrastructure came together. In close collaboration with Fraunhofer IAIS and TU Dortmund, we focused on foundation model technologies themselves. The JQL pipeline paper and our TACL paper on pruning efficiency address controllability and efficiency—prerequisites for serious open foundation models.

This is also where hybrid architectures come in. Looking forward, I see strong potential in combining state-space models with neural foundation models, especially for long-horizon reasoning and dynamics. These hybrid approaches naturally belong in the open foundation model space, where architectures, assumptions, and failure modes must remain inspectable.

Institutionally, this work is embedded in the Lamarr Institute, and via Fraunhofer connected to broader OSFM initiatives and the JAIF AI Factory. This alignment with shared compute, tooling, and deployment pathways makes it realistic to maintain and evolve models beyond individual projects—and reflects a broader conviction that trustworthy AI requires shared infrastructure.

### **People, Transitions, and the Next Phase**

This year also marks a significant transition for the team. With the successful completion of five BMBF projects, several colleagues will be moving on at the end of the year—something that always feels bittersweet, but also reflects the training role of the group.

At the same time, we are entering a growth phase. Over the coming months, the group will welcome four new researchers in AI for Science and four new researchers funded through the ERC, complemented by Florian Mai’s junior research group on AI safety, which will work closely with Nicolas Kluge. Together, these efforts fit squarely into Lamarr’s overarching theme of AI trustworthiness, spanning social alignment, scientific reliability, and open, inspectable foundations.

### **Outreach and Research Community**

Alongside research, the past year also emphasized community engagement and outreach. We helped shape international discourse by organizing INLG, strengthened transatlantic exchange through visits to Canadian AI institutes, and actively participated in AI policy and public debates across Germany, from Bonn to Berlin. In addition, serving on the jury of the BWKI Wettbewerb allowed us to engage directly with young AI talent at an early stage. Together, these activities reflect a commitment to cultivating the research ecosystem across generations and borders, alongside advancing the science itself.

### **Looking Forward**

If there is one conviction guiding our next phase, it is this:

**The next phase of AI will not be defined by scale alone, but by whether we can align powerful systems with human judgment, scientific rigor, and institutional responsibility.**

With socially aligned AI, physics foundation models, scientific agents, hybrid architectures, and open infrastructures coming together, we are now in a position to ask not just what AI can do, but what it should be trusted to do—and under which conditions. That is the question we are choosing to work on now, while the technology works well enough for the answer to matter.



**To our dedicated researchers, collaborators, and supporters: thank you for your unwavering commitment to pushing the boundaries of human knowledge. Here's to another year of curiosity, innovation, and transformative research!**

{% include figure.html image="images/CaisaLamarr.jpeg" caption="" width="550px" %}
{% include figure.html image="images/MLprague.jpeg" caption="" width="550px" %}

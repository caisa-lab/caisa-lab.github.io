---
title: Two papers with contributions of CAISA Lab members accepted to EACL 2021 
tags: 
  - EACL
  - User Representations
  - Conversational Question Answering
---

What a great start of the year! We are delighted that two research full-papers with the contributions of our lab members have been accepted to EACL '21, the 16th Conference of the European Chapter of the Association for Computational Linguistics.

1. "PHASE: Learning Emotional Phase-aware Representations for Suicide Ideation Detection on Social Media" from Ramit Shawney, Harshit Joshi, Lucie Flek, and Rajiv Ratn Shah

2. "Conversational Question Answering over Knowledge Graphs with Transformer and Graph Attention Networks" from Joan Plepi, Endri Kacupaj, Kuldeep Singh, Harsh Thakkar, Jens Lehmann and Maria Maleshkova

In the paper on phase-aware representations, originating from our collaboration with the MIDAS Lab at IIT Delhi, we focus on identifying suicidal intent in tweets by augmenting linguistic models with emotional phases modeled from users' historical context. We propose PHASE, a time-and phase-aware framework that adaptively learns features from a user's historical emotional spectrum on Twitter for preliminary screening of suicidal risk. Building on clinical studies, PHASE learns phase-like progressions in users' historical Plutchik-wheel-based emotions to contextualize suicidal intent. While outperforming state-of-the-art methods, we show the utility of temporal and phase-based emotional contextual cues for suicide ideation detection.
 
In the paper on conversational question answering, transformer architecture is used to extract the conversational context and map questions to executable logical forms. The work is building upon the master thesis research of Joan, 
showing that semantic parsing approach is effective on the CSQA task, transformer architecture improves contextual embeddings, and decoder extension with graph networks help to select the correct knowledge base items.

We are looking forward to presenting the work at EACL 2021!

